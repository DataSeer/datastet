version: "0.7.2-SNAPSHOT"

corpusPath: "./resources/dataset/dataseer/corpus"
templatePath: "./resources/dataset/dataseer/crfpp-templates/dataseer.template"
grobidHome: "../grobid-home"
tmpPath: "tmp/"

# path to Pub2TEI repository as available at https://github.com/kermitt2/Pub2TEI
pub2teiPath: "../../Pub2TEI/"

gluttonHost: "https://cloud.science-miner.com/glutton"
gluttonPort: 

# if true we use binary classifiers for the contexts, otherwise use a single multi-label classifier
# binary classifiers perform better, but havier to use
useBinaryContextClassifiers: true

# sequence labeling model (identify data-related sections)
models:

  # model for zones
  - name: "dataseer"
    engine: "wapiti"
    #engine: "delft"
    wapiti:
      # wapiti training parameters, they will be used at training time only
      epsilon: 0.00001
      window: 20
      nbMaxIterations: 2000

  # model for dataset mention recognition
  - name: "dataseer-mention"
    engine: "wapiti"
    #engine: "delft"
    wapiti:
      # wapiti training parameters, they will be used at training time only
      epsilon: 0.00001
      window: 20
      nbMaxIterations: 2000
    delft:
      # deep learning parameters
      architecture: "BidLSTM_CRF"
      #architecture: "scibert"
      useELMo: false
      embeddings_name: "glove-840B"

  # classifier model, dataset binary (datset or not dataset in the current sentence)
  - name: "dataseer-binary"
    engine: "delft"
    delft:
      # deep learning parameters
      architecture: "gru"
      #architecture: "bert"
      embeddings_name: "word2vec"
      #transformer: "allenai/scibert_scivocab_cased"

  # identification of the data type (first level hierarchy) 
  - name: "dataseer-first"
    engine: "delft"
    delft:
      # deep learning parameters
      architecture: "gru"
      #architecture: "bert"
      embeddings_name: "word2vec"
      #transformer: "allenai/scibert_scivocab_cased"

  # mention context classification (reuse binary for the moment)
  - name: "dataseer-reuse"
    engine: "delft"
    delft:
      # deep learning parameters
      architecture: "gru"
      #architecture: "bert"
      embeddings_name: "word2vec"
      #transformer: "allenai/scibert_scivocab_cased"

  # model for dataset mention recognition
  - name: "datasets"
    #engine: "wapiti"
    engine: "delft"
    wapiti:
      # wapiti training parameters, they will be used at training time only
      epsilon: 0.00001
      window: 20
      nbMaxIterations: 2000
    delft:
      # deep learning parameters
      architecture: "BidLSTM_CRF"
      #architecture: "scibert"
      useELMo: true
      embeddings_name: "glove-840B"
      runtime:
        # parameters used at runtime/prediction
        max_sequence_length: 300
        batch_size: 10

  - name: "dataset_context"
    engine: "delft"
    delft:
      #architecture: "gru"
      #embeddings_name: "glove-840B"
      architecture: "bert"
      transformer: "allenai/scibert_scivocab_cased"

  - name: "dataset_context_used"
    engine: "delft"
    delft:
      #architecture: "gru"
      #embeddings_name: "glove-840B"
      architecture: "bert"
      transformer: "allenai/scibert_scivocab_cased"

  - name: "dataset_context_creation"
    engine: "delft"
    delft:
      #architecture: "gru"
      #embeddings_name: "glove-840B"
      architecture: "bert"
      transformer: "allenai/scibert_scivocab_cased"

  - name: "dataset_context_shared"
    engine: "delft"
    delft:
      #architecture: "gru"
      #embeddings_name: "glove-840B"
      architecture: "bert"
      transformer: "allenai/scibert_scivocab_cased"
